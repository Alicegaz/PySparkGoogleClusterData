{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "import time\n",
    "from pyspark.sql.functions import countDistinct\n",
    "import timeit\n",
    "import gc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_values(init_time, finish_time, granularity):\n",
    "    init_time = init_time*10**6\n",
    "    finish_time = finish_time*10**6\n",
    "    granularity = granularity*10**6\n",
    "    return init_time, finish_time, granularity\n",
    "\n",
    "def calculate_tuples(cluster_list):\n",
    "    tuples = []\n",
    "    for i, e in enumerate(cluster_list):\n",
    "        if i < len(cluster_list)-1:\n",
    "            tuples.append((e, cluster_list[i+1]))\n",
    "    return tuples\n",
    "\n",
    "def mean_time_evaluation(sc, cluster_list):\n",
    "    tuples = calculate_tuples(cluster_list)\n",
    "    tuples_RDD = sc.parallelize(tuples)\n",
    "    \n",
    "    tuples_RDD = tuples_RDD.map(lambda elem: (elem[1] - elem[0]))\n",
    "    mean_value = numpy.mean(tuples_RDD.collect())\n",
    "    return mean_value\n",
    "\n",
    "def get_interval_values(sc, cluster_list, init_time, finish_time):\n",
    "    tuples = calculate_tuples(cluster_list)\n",
    "    interval = sc.parallelize(tuples)\n",
    "    interval = interval.map(lambda elem: (init_time, finish_time, elem[0], elem[1], (elem[1]-elem[0])/10**6))\n",
    "    return interval.collect()\n",
    "\n",
    "def metrics(inter_list):\n",
    "    if inter_list.count > 0:\n",
    "        return [np.mean(inter_list), np.var(inter_list), np.median(inter_list), np.std(inter_list)]\n",
    "    else:\n",
    "        return [0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[1]\")\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_events = pd.read_csv(\"../../datasets/job_events/part-00000-of-00500.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. timestamp 1. missing info 2. job ID 3. event type 4. user name 5. scheduling class 6. job name 7. logical job name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: ['1639265380304,,6408251525,4,jVEIdGnEYLp+j9YJHh5dEBhUdpD2fs+PKTWwQo5ZrJk=,1,zXOstV9tbQGnXU6u6FfAjOzoe8gToH3sDCKZNm0RAwM=,1H3WBVcen2RS9lximFVb5A/HIAxc6rH8XBp0IStE/Co=']\n",
      "Step 2: [['1639265380304', '', '6408251525', '4', 'jVEIdGnEYLp+j9YJHh5dEBhUdpD2fs+PKTWwQo5ZrJk=', '1', 'zXOstV9tbQGnXU6u6FfAjOzoe8gToH3sDCKZNm0RAwM=', '1H3WBVcen2RS9lximFVb5A/HIAxc6rH8XBp0IStE/Co=']]\n",
      "Step 3: [(1639265380304, 6408251525, 4, 'jVEIdGnEYLp+j9YJHh5dEBhUdpD2fs+PKTWwQo5ZrJk=', 1, 'zXOstV9tbQGnXU6u6FfAjOzoe8gToH3sDCKZNm0RAwM=', '1H3WBVcen2RS9lximFVb5A/HIAxc6rH8XBp0IStE/Co=')]\n"
     ]
    }
   ],
   "source": [
    "job_events = sc.textFile('../../datasets/job_events/*.gz')\n",
    "print(\"Step 1: {}\".format(job_events.take(1)))\n",
    "job_events_RDD = job_events.map(lambda line: line.split(\",\"))\n",
    "print(\"Step 2: {}\".format(job_events_RDD.take(1)))\n",
    "\n",
    "job_events_RDD = job_events_RDD.map(lambda tokens: (int(tokens[0]), int(tokens[2]), int(tokens[3]), tokens[4], int(tokens[5]), tokens[6], tokens[7]))\n",
    "print(\"Step 3: {}\".format(job_events_RDD.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7970 traces removed\n",
      "Step 4: [(604046276, (6251639640, 4))]\n"
     ]
    }
   ],
   "source": [
    "#remove from the traces records occured before the beginning of the trace window (timestamp=0)\n",
    "#remove records occured after the end of the trace window (timestamp=2^63-1)\n",
    "#sort in accending order with respect to timestamps\n",
    "job_events_RDD_filtered = job_events_RDD.map(lambda elem: (elem[0], (elem[1], elem[2]))).filter(lambda elem: elem[0] != 0 and elem[0] != (2^63-1)).sortByKey(1, 1)\n",
    "#sortByKey(accending=True, number of partitions)\n",
    "print(\"{} traces removed\".format(job_events_RDD.count()-job_events_RDD_filtered.count()))\n",
    "print(\"Step 4: {}\".format(job_events_RDD_filtered.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: \n",
    "    ##event_type: type of the event in the traces\n",
    "    ##init_time: time starting from which we want to evaluate the model\n",
    "    ##finish_time: it is the time when we want to stop the evaluation\n",
    "    ##granularity: the level of granularity for plotting the results of the model\n",
    "    ##over a window of 200 sec, granularity = 10 sec ==> derived traces will be clustered based on the timestamps\n",
    "    ##EX: cluster 1: time interval 0-10, cluster 2: time interval 10-20 ... cluster 20: time interval 180-200\n",
    "\n",
    "def job_eval_time_window(event_type, init_time, finish_time, granularity):\n",
    "    init_time, finish_time, granularity = adjust_values(init_time, finish_time, granularity)\n",
    "    if (event_type != None):\n",
    "        job_eval_traces_RDD = job_events_RDD_filtered.filter(lambda elem: elem[1][1] == event_type)\n",
    "    if (init_time!=None and finish_time!=None):\n",
    "        job_eval_traces_RDD = job_eval_traces_RDD.filter(lambda elem: elem[0]>=init_time and elem[0]<=finish_time).map(lambda elem: elem[0])\n",
    "    print(\"{} Traces satify query\".format(job_eval_traces_RDD.count()))\n",
    "    job_eval_traces_list = job_eval_traces_RDD.collect()\n",
    "    job_evaluated_means_list = []\n",
    "    lower_granularity = init_time #lowest bound for the clusterization\n",
    "    n_clusters = (finish_time - init_time)/granularity\n",
    "    job_interval_values_list = []\n",
    "    \n",
    "    #propagate the last element of each cluster to be the first element of the following cluster\n",
    "    last_element = 0\n",
    "    n_empty_clusters = 0\n",
    "    clusters_metrics = []\n",
    "    \n",
    "    #iterate to create each cluster\n",
    "    for i in range(0, int(n_clusters+1)):\n",
    "        j_cluster_traces = [timestamp for timestamp in job_eval_traces_list if timestamp >= lower_granularity and timestamp < (lower_granularity+granularity)]\n",
    "        \n",
    "        if len(j_cluster_traces) == 0:\n",
    "            \n",
    "        if i!=0:\n",
    "            j_cluster_traces.insert(0, last_element)\n",
    "        \n",
    "        #append to a list the cluster (cluster_lower_bound, cluster_upper_bound, mean_time_between_jobs)\n",
    "        job_evaluated_means_list.append([lower_granularity, (lower_granularity+granularity), mean_time_evaluation(sc, j_cluster_traces)])\n",
    "        \n",
    "        #list of all interarrival times inside the cluster\n",
    "        interval_values_list = get_interval_values(sc, j_cluster_traces, lower_granularity, lower_granularity+granularity)\n",
    "        inter_arrivals = sc.parallelize(interval_values_list).map(lambda x: x[4])\n",
    "        #Evaluate the metrics over the interarrivals of the cluster\n",
    "        #[mean, variance, median, std]\n",
    "        cluster_metrics = metrics(inter_arrivals.collect())\n",
    "        \n",
    "        if (math.isnan(cluster_metrics[0]) == False):\n",
    "            clusters_metrics.append(cluster_metrics)\n",
    "        job_interval_values_list.extend(interval_values_list)\n",
    "        lower_granularity += granularity\n",
    "        last_element = j_cluster_traces[-1]\n",
    "        \n",
    "    metrics_list = evaluate_statistics(sc, job_interval_values_list)\n",
    "    job_mean_metrics_rdd = sc.parallelize(job_clusters_metrics)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
